{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Instalacion y Carga Previa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['article', 'highlights', 'id'],\n",
      "    num_rows: 11490\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "ds = load_dataset(\"abisee/cnn_dailymail\", \"3.0.0\",  split=\"test\")\n",
    "print(ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Añadido 'c:\\fespa-dev\\nlp-curso\\nlp-proyecto03' a sys.path\n"
     ]
    }
   ],
   "source": [
    "import os, sys\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "    print(f\"Añadido '{module_path}' a sys.path\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using transformers v4.51.3\n",
      "Using datasets v3.6.0\n"
     ]
    }
   ],
   "source": [
    "from src.utils import *\n",
    "setup_chapter()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* devive: Nos aseguramos que estemos usando el poder computacional de nuestra GPU ;p\n",
    "* model: Modelo preentrenado, en este caso usaremos un modelo preentrado `gpt2-xl`\n",
    "* tokenizer: Tokenizador para nuestro modelo, se ajusta segun el modelo seleccionado\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model_name = \"gpt2-xl\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT2TokenizerFast(name_or_path='gpt2-xl', vocab_size=50257, model_max_length=1024, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>'}, clean_up_tokenization_spaces=False, added_tokens_decoder={\n",
       "\t50256: AddedToken(\"<|endoftext|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n",
       "}\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Greedy Search Decoding --> torch implementaion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "def greedy_search_generation(model, tokenizer, input_txt):\n",
    "    iterations = []\n",
    "    n_steps = 8\n",
    "    choices_per_step = 5\n",
    "\n",
    "    input_ids = tokenizer(input_txt, return_tensors=\"pt\")[\"input_ids\"].to(device)\n",
    "    with torch.no_grad():\n",
    "        for _ in range(n_steps):\n",
    "            iteration = dict()\n",
    "            iteration[\"Input\"] = tokenizer.decode(input_ids[0])\n",
    "            output = model(input_ids=input_ids)\n",
    "            # Seleccionamos los logits del primer batch y el último token y aplicamos softmax\n",
    "            next_token_logits = output.logits[0, -1, :]\n",
    "            next_token_probs = torch.softmax(next_token_logits, dim=-1)\n",
    "            sorted_ids = torch.argsort(next_token_probs, dim=-1, descending=True)\n",
    "            # Guardamos los tokens con mayor probabilidad :>\n",
    "            for choice_idx in range(choices_per_step):\n",
    "                token_id = sorted_ids[choice_idx]\n",
    "                token_prob = next_token_probs[token_id].cpu().numpy()\n",
    "                token_choice = (\n",
    "                    f\"{tokenizer.decode(token_id)} ({100 * token_prob:.2f}%)\"\n",
    "                )\n",
    "                iteration[f\"Choice {choice_idx+1}\"] = token_choice\n",
    "            # Insertamos el token predicho a nuestro input para la siguiente iteración\n",
    "            input_ids = torch.cat([input_ids, sorted_ids[None, 0, None]], dim=-1)\n",
    "            iterations.append(iteration)\n",
    "    return iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Input</th>\n",
       "      <th>Choice 1</th>\n",
       "      <th>Choice 2</th>\n",
       "      <th>Choice 3</th>\n",
       "      <th>Choice 4</th>\n",
       "      <th>Choice 5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Once upon a time, in a land far, far away, the...</td>\n",
       "      <td>eat (14.80%)</td>\n",
       "      <td>fly (6.00%)</td>\n",
       "      <td>play (4.72%)</td>\n",
       "      <td>read (4.45%)</td>\n",
       "      <td>sing (3.22%)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Once upon a time, in a land far, far away, the...</td>\n",
       "      <td>. (13.28%)</td>\n",
       "      <td>people (11.11%)</td>\n",
       "      <td>dragons (3.08%)</td>\n",
       "      <td>humans (2.38%)</td>\n",
       "      <td>the (2.10%)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Once upon a time, in a land far, far away, the...</td>\n",
       "      <td>He (12.89%)</td>\n",
       "      <td>One (12.45%)</td>\n",
       "      <td>\\n (11.91%)</td>\n",
       "      <td>His (5.39%)</td>\n",
       "      <td>The (4.74%)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Once upon a time, in a land far, far away, the...</td>\n",
       "      <td>had (22.78%)</td>\n",
       "      <td>was (16.69%)</td>\n",
       "      <td>loved (8.75%)</td>\n",
       "      <td>lived (5.97%)</td>\n",
       "      <td>would (3.87%)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Once upon a time, in a land far, far away, the...</td>\n",
       "      <td>a (36.92%)</td>\n",
       "      <td>many (18.03%)</td>\n",
       "      <td>three (5.50%)</td>\n",
       "      <td>two (5.39%)</td>\n",
       "      <td>one (2.75%)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Once upon a time, in a land far, far away, the...</td>\n",
       "      <td>lot (5.06%)</td>\n",
       "      <td>very (3.61%)</td>\n",
       "      <td>great (2.82%)</td>\n",
       "      <td>dragon (2.31%)</td>\n",
       "      <td>large (2.27%)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Once upon a time, in a land far, far away, the...</td>\n",
       "      <td>of (97.88%)</td>\n",
       "      <td>to (1.27%)</td>\n",
       "      <td>, (0.19%)</td>\n",
       "      <td>. (0.12%)</td>\n",
       "      <td>more (0.10%)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Once upon a time, in a land far, far away, the...</td>\n",
       "      <td>friends (52.05%)</td>\n",
       "      <td>dragon (4.48%)</td>\n",
       "      <td>food (3.58%)</td>\n",
       "      <td>dragons (3.21%)</td>\n",
       "      <td>followers (2.05%)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Input           Choice 1  \\\n",
       "0  Once upon a time, in a land far, far away, the...       eat (14.80%)   \n",
       "1  Once upon a time, in a land far, far away, the...         . (13.28%)   \n",
       "2  Once upon a time, in a land far, far away, the...        He (12.89%)   \n",
       "3  Once upon a time, in a land far, far away, the...       had (22.78%)   \n",
       "4  Once upon a time, in a land far, far away, the...         a (36.92%)   \n",
       "5  Once upon a time, in a land far, far away, the...        lot (5.06%)   \n",
       "6  Once upon a time, in a land far, far away, the...        of (97.88%)   \n",
       "7  Once upon a time, in a land far, far away, the...   friends (52.05%)   \n",
       "\n",
       "           Choice 2          Choice 3          Choice 4            Choice 5  \n",
       "0       fly (6.00%)      play (4.72%)      read (4.45%)        sing (3.22%)  \n",
       "1   people (11.11%)   dragons (3.08%)    humans (2.38%)         the (2.10%)  \n",
       "2      One (12.45%)       \\n (11.91%)       His (5.39%)         The (4.74%)  \n",
       "3      was (16.69%)     loved (8.75%)     lived (5.97%)       would (3.87%)  \n",
       "4     many (18.03%)     three (5.50%)       two (5.39%)         one (2.75%)  \n",
       "5      very (3.61%)     great (2.82%)    dragon (2.31%)       large (2.27%)  \n",
       "6        to (1.27%)         , (0.19%)         . (0.12%)        more (0.10%)  \n",
       "7    dragon (4.48%)      food (3.58%)   dragons (3.21%)   followers (2.05%)  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_txt = \"Once upon a time, in a land far, far away, there lived a dragon who loved to\"\n",
    "iterations = greedy_search_generation(model, tokenizer, input_txt)\n",
    "df = pd.DataFrame(iterations)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Input': 'Once upon a time, in a land far, far away, there lived a dragon who loved to eat. He had a lot of',\n",
       " 'Choice 1': ' friends (52.05%)',\n",
       " 'Choice 2': ' dragon (4.48%)',\n",
       " 'Choice 3': ' food (3.58%)',\n",
       " 'Choice 4': ' dragons (3.21%)',\n",
       " 'Choice 5': ' followers (2.05%)'}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iterations[-1]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
